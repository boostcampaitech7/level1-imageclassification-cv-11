{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af8a2c0-45fe-4d13-bebd-0dca87a7b71f",
   "metadata": {
    "papermill": {
     "duration": 0.003356,
     "end_time": "2024-09-20T15:15:24.224884",
     "exception": false,
     "start_time": "2024-09-20T15:15:24.221528",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "412ec593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.10.0\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print(cv2.__version__)\n",
    "print(hasattr(cv2, 'ximgproc'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4c611c8-2226-433c-bf5f-343cc0b094af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:24.236987Z",
     "iopub.status.busy": "2024-09-20T15:15:24.236809Z",
     "iopub.status.idle": "2024-09-20T15:15:26.191986Z",
     "shell.execute_reply": "2024-09-20T15:15:26.191502Z"
    },
    "papermill": {
     "duration": 1.959609,
     "end_time": "2024-09-20T15:15:26.193089",
     "exception": false,
     "start_time": "2024-09-20T15:15:24.233480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/blueberry/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 필요 library들을 import합니다.\n",
    "import os\n",
    "from typing import Tuple, Any, Callable, List, Optional, Union\n",
    "\n",
    "import cv2\n",
    "import timm\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models, datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from torchcam.methods import GradCAM\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f28d0864",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.205531Z",
     "iopub.status.busy": "2024-09-20T15:15:26.205317Z",
     "iopub.status.idle": "2024-09-20T15:15:26.234289Z",
     "shell.execute_reply": "2024-09-20T15:15:26.233960Z"
    },
    "papermill": {
     "duration": 0.038972,
     "end_time": "2024-09-20T15:15:26.235092",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.196120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5e9123b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.241375Z",
     "iopub.status.busy": "2024-09-20T15:15:26.241249Z",
     "iopub.status.idle": "2024-09-20T15:15:26.244249Z",
     "shell.execute_reply": "2024-09-20T15:15:26.244040Z"
    },
    "papermill": {
     "duration": 0.007367,
     "end_time": "2024-09-20T15:15:26.245380",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.238013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d69e6a-a719-4a97-92ca-6354c873313f",
   "metadata": {
    "papermill": {
     "duration": 0.002761,
     "end_time": "2024-09-20T15:15:26.251213",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.248452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c791fc24",
   "metadata": {},
   "source": [
    "## BPD & MSR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4fe72f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def bezier_pivot_deformation(image, num_deformations=6, deformation_degree=10): # 원본 이미지로부터 6개의 변형된 이미지를 생성\n",
    "    \"\"\"\n",
    "    Applies Bezier pivot based deformation (BPD) on a sketch image.\n",
    "\n",
    "    Args:\n",
    "    - image (numpy.ndarray): The input sketch image.\n",
    "    - num_deformations (int): Number of deformations to generate.\n",
    "    - deformation_degree (int): The degree of random shift for control pivots.\n",
    "    \n",
    "    Returns:\n",
    "    - List of deformed images.\n",
    "    \"\"\"\n",
    "    def fit_bezier_curve(points):\n",
    "        # Fit a cubic Bezier curve to the points using least squares method\n",
    "        n = len(points)\n",
    "        t = np.linspace(0, 1, n)\n",
    "        phi = 1 - t\n",
    "        A = np.vstack([phi**3, 3*t*phi**2, 3*t**2*phi, t**3]).T\n",
    "        B = np.linalg.lstsq(A, points, rcond=None)[0]\n",
    "        return B\n",
    "\n",
    "    def deform_curve(p0, p1, p2, p3, alpha=deformation_degree):\n",
    "        # Randomly deform the control pivots\n",
    "        delta = np.random.uniform(-alpha, alpha, size=(2,))\n",
    "        return p0, p1 + delta, p2 + delta, p3\n",
    "\n",
    "    # Step 1: Convert image to binary and skeletonize\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    _, binary = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)\n",
    "    skeleton = cv2.ximgproc.thinning(binary)\n",
    "\n",
    "    # Step 2: Segment into patches and deform curves\n",
    "    patches = []\n",
    "    patch_size = 32\n",
    "    h, w = skeleton.shape\n",
    "    for y in range(0, h, patch_size):\n",
    "        for x in range(0, w, patch_size):\n",
    "            patch = skeleton[y:y + patch_size, x:x + patch_size]\n",
    "            patches.append(patch)\n",
    "\n",
    "    deformed_images = []\n",
    "    for _ in range(num_deformations):\n",
    "        new_image = np.zeros_like(skeleton)\n",
    "        for patch in patches:\n",
    "            # Fit Bezier curve to patch\n",
    "            y, x = np.where(patch > 0)\n",
    "            if len(x) > 3:\n",
    "                points = np.column_stack((x, y))\n",
    "                p0, p1, p2, p3 = fit_bezier_curve(points)\n",
    "\n",
    "                # Deform curve using random shifts\n",
    "                p0, p1, p2, p3 = deform_curve(p0, p1, p2, p3)\n",
    "\n",
    "                # Reconstruct the patch with deformed curve\n",
    "                new_image[y, x] = 255\n",
    "        deformed_images.append(new_image)\n",
    "\n",
    "    return deformed_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd66958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 패치 추출 및 HOG 특징 계산 함수\n",
    "def extract_patches_and_hog_features(image, patch_size=31):\n",
    "    \"\"\"\n",
    "    주어진 이미지에서 패치를 추출하고, 각 패치의 HOG 특징을 계산하는 함수.\n",
    "    \n",
    "    Args:\n",
    "    - image (numpy.ndarray): 입력 이미지.\n",
    "    - patch_size (int): 패치의 크기.\n",
    "    \n",
    "    Returns:\n",
    "    - patches (List): 패치의 리스트.\n",
    "    - hog_features (List): HOG 특징의 리스트.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    _, binary = cv2.threshold(gray, 128, 255, cv2.THRESH_BINARY)\n",
    "    skeleton = cv2.ximgproc.thinning(binary)\n",
    "\n",
    "    h, w = skeleton.shape\n",
    "    patches = []\n",
    "    hog_features = []\n",
    "    \n",
    "    for y in range(0, h, patch_size):\n",
    "        for x in range(0, w, patch_size):\n",
    "            patch = skeleton[y:y + patch_size, x:x + patch_size]\n",
    "            if patch.shape[0] == patch_size and patch.shape[1] == patch_size:  # 패치 크기 확인\n",
    "                patches.append((patch, (x, y)))\n",
    "                hog_feature = hog(patch, pixels_per_cell=(8, 8), cells_per_block=(1, 1), feature_vector=True)\n",
    "                hog_features.append(hog_feature)\n",
    "    \n",
    "    return patches, hog_features\n",
    "\n",
    "# 클러스터 중심 계산 함수\n",
    "def compute_cluster_centers(images, n_clusters=150, patch_size=31):\n",
    "    \"\"\"\n",
    "    여러 이미지에서 HOG 특징을 추출하고, 이를 KMeans로 클러스터링하여 클러스터 중심을 계산.\n",
    "    \n",
    "    Args:\n",
    "    - images (List of np.ndarray): 학습 이미지들의 리스트.\n",
    "    - n_clusters (int): 클러스터 개수.\n",
    "    - patch_size (int): 각 패치의 크기.\n",
    "    \n",
    "    Returns:\n",
    "    - cluster_centers (np.ndarray): 계산된 클러스터 중심.\n",
    "    \"\"\"\n",
    "    all_hog_features = []\n",
    "    \n",
    "    # 모든 이미지에서 패치와 HOG 특징 추출\n",
    "    for image in images:\n",
    "        _, hog_features = extract_patches_and_hog_features(image, patch_size)\n",
    "        all_hog_features.extend(hog_features)\n",
    "    \n",
    "    # KMeans로 클러스터링\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(all_hog_features)\n",
    "    \n",
    "    return kmeans.cluster_centers_\n",
    "\n",
    "# MSR 함수\n",
    "def mean_stroke_reconstruction(image, cluster_centers, patch_size=31):\n",
    "    \"\"\"\n",
    "    계산된 클러스터 중심을 사용하여 입력 이미지에 Mean Stroke Reconstruction을 적용하는 함수.\n",
    "    \n",
    "    Args:\n",
    "    - image (numpy.ndarray): 입력 스케치 이미지.\n",
    "    - cluster_centers (List): 미리 계산된 클러스터 중심 리스트.\n",
    "    - patch_size (int): 패치 크기.\n",
    "    \n",
    "    Returns:\n",
    "    - Reconstructed image (numpy.ndarray): 재구성된 이미지.\n",
    "    \"\"\"\n",
    "    patches, hog_features = extract_patches_and_hog_features(image, patch_size)\n",
    "    \n",
    "    # 각 패치를 클러스터 중심으로 대체\n",
    "    kmeans = KMeans(n_clusters=len(cluster_centers)).fit(hog_features)\n",
    "    labels = kmeans.predict(hog_features)\n",
    "\n",
    "    new_image = np.zeros_like(patches[0][0])\n",
    "    \n",
    "    for i, (patch, (x, y)) in enumerate(patches):\n",
    "        mean_stroke = cluster_centers[labels[i]]\n",
    "        new_image[y:y + patch_size, x:x + patch_size] = np.reshape(mean_stroke, (patch_size, patch_size))\n",
    "\n",
    "    return new_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c47a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bpd_with_partial_msr(image, cluster_centers, msr_ratio=0.5, num_deformations=6):\n",
    "    \"\"\"\n",
    "    BPD로 변형된 이미지 중 일부에만 MSR을 적용하는 함수.\n",
    "    \n",
    "    Args:\n",
    "    - image (numpy.ndarray): 원본 스케치 이미지.\n",
    "    - cluster_centers (List): MSR에서 사용할 클러스터 중심.\n",
    "    - msr_ratio (float): MSR을 적용할 이미지 비율 (0.0 ~ 1.0).\n",
    "    - num_deformations (int): BPD로 생성할 이미지 개수.\n",
    "    \n",
    "    Returns:\n",
    "    - List of transformed images, where a portion of them have MSR applied.\n",
    "    \"\"\"\n",
    "    # 1. BPD를 적용하여 여러 개의 변형된 이미지 생성\n",
    "    deformed_images = bezier_pivot_deformation(image, num_deformations=num_deformations)\n",
    "    \n",
    "    # 2. MSR 적용 여부 결정\n",
    "    num_msr = int(msr_ratio * num_deformations)  # MSR을 적용할 이미지 개수\n",
    "    msr_images = random.sample(deformed_images, num_msr)  # 무작위로 선택\n",
    "    \n",
    "    final_images = []\n",
    "    for img in deformed_images:\n",
    "        # NumPy 배열 비교 수정\n",
    "        if any(np.array_equal(img, msr_img) for msr_img in msr_images):\n",
    "            # MSR 적용\n",
    "            img = mean_stroke_reconstruction(img, cluster_centers)\n",
    "        final_images.append(img)\n",
    "    \n",
    "    return final_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56f97229-e29f-479d-abab-0db8219d1803",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.257261Z",
     "iopub.status.busy": "2024-09-20T15:15:26.257134Z",
     "iopub.status.idle": "2024-09-20T15:15:26.260720Z",
     "shell.execute_reply": "2024-09-20T15:15:26.260431Z"
    },
    "papermill": {
     "duration": 0.007613,
     "end_time": "2024-09-20T15:15:26.261587",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.253974",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        root_dir: str, \n",
    "        info_df: pd.DataFrame, \n",
    "        transform: Callable,\n",
    "        cluster_centers: np.ndarray,  # MSR에 사용할 클러스터 중심 추가\n",
    "        is_inference: bool = False,\n",
    "        apply_bpd_msr: bool = False  # BPD와 MSR 적용 여부\n",
    "    ):\n",
    "        # 데이터셋의 기본 경로, 이미지 변환 방법, 이미지 경로 및 레이블을 초기화합니다.\n",
    "        self.root_dir = root_dir  # 이미지 파일들이 저장된 기본 디렉토리\n",
    "        self.transform = transform  # 이미지에 적용될 변환 처리\n",
    "        self.cluster_centers = cluster_centers  # MSR에 사용할 클러스터 중심\n",
    "        self.is_inference = is_inference # 추론인지 확인\n",
    "        self.apply_bpd_msr = apply_bpd_msr  # BPD와 MSR 적용 여부\n",
    "        self.image_paths = info_df['image_path'].tolist()  # 이미지 파일 경로 목록\n",
    "        \n",
    "        if not self.is_inference:\n",
    "            self.targets = info_df['target'].tolist()  # 각 이미지에 대한 레이블 목록\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # 데이터셋의 총 이미지 수를 반환합니다.\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Union[Tuple[torch.Tensor, int], torch.Tensor]:\n",
    "        # 주어진 인덱스에 해당하는 이미지를 로드하고 변환을 적용한 후, 이미지와 레이블을 반환합니다.\n",
    "        img_path = os.path.join(self.root_dir, self.image_paths[index])  # 이미지 경로 조합\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)  # 이미지를 BGR 컬러 포맷의 numpy array로 읽어옵니다.\n",
    "        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # BGR 포맷을 RGB 포맷으로 변환합니다.\n",
    "        \n",
    "        if len(image.shape) == 2:  # 1채널 이미지일 경우\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)  # 1채널 이미지를 3채널로 변환\n",
    "        else:  # 3채널 이미지일 경우\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # BGR을 RGB로 변환\n",
    "        \n",
    "        \n",
    "        # BPD와 MSR을 적용하는 부분 추가\n",
    "        if self.apply_bpd_msr:\n",
    "            image = apply_bpd_with_partial_msr(\n",
    "                image, \n",
    "                self.cluster_centers, \n",
    "                msr_ratio=0.5,  # MSR을 50%만 적용\n",
    "                num_deformations=6\n",
    "            )\n",
    "            image = random.choice(image)  # 변형된 이미지 중 하나 선택\n",
    "        \n",
    "        \n",
    "        image = self.transform(image)  # 설정된 이미지 변환을 적용합니다.# Albumentations 또는 기타 transform 적용\n",
    "\n",
    "        if self.is_inference:\n",
    "            return image\n",
    "        else:\n",
    "            target = self.targets[index]  # 해당 이미지의 레이블\n",
    "            return image, target  # 변환된 이미지와 레이블을 튜플 형태로 반환합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c07d2d0-9585-45ce-8ece-4f69b98f6dd4",
   "metadata": {
    "papermill": {
     "duration": 0.00274,
     "end_time": "2024-09-20T15:15:26.267139",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.264399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transform Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b1855c1-cf13-476d-aabd-d78e9e082ddf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.273463Z",
     "iopub.status.busy": "2024-09-20T15:15:26.273265Z",
     "iopub.status.idle": "2024-09-20T15:15:26.276568Z",
     "shell.execute_reply": "2024-09-20T15:15:26.276267Z"
    },
    "papermill": {
     "duration": 0.007336,
     "end_time": "2024-09-20T15:15:26.277179",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.269843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TorchvisionTransform:\n",
    "    def __init__(self, is_train: bool = True):\n",
    "        # 공통 변환 설정: 이미지 리사이즈, 텐서 변환, 정규화\n",
    "        common_transforms = [\n",
    "            transforms.Resize((224, 224)),  # 이미지를 224x224 크기로 리사이즈\n",
    "            transforms.ToTensor(),  # 이미지를 PyTorch 텐서로 변환\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 정규화\n",
    "        ]\n",
    "        \n",
    "        if is_train:\n",
    "            # 훈련용 변환: 랜덤 수평 뒤집기, 랜덤 회전, 색상 조정 추가\n",
    "            self.transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.RandomHorizontalFlip(p=0.5),  # 50% 확률로 이미지를 수평 뒤집기\n",
    "                    transforms.RandomRotation(15),  # 최대 15도 회전\n",
    "                    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # 밝기 및 대비 조정\n",
    "                ] + common_transforms\n",
    "            )\n",
    "        else:\n",
    "            # 검증/테스트용 변환: 공통 변환만 적용\n",
    "            self.transform = transforms.Compose(common_transforms)\n",
    "\n",
    "    def __call__(self, image: np.ndarray) -> torch.Tensor:\n",
    "        image = Image.fromarray(image)  # numpy 배열을 PIL 이미지로 변환\n",
    "        \n",
    "        transformed = self.transform(image)  # 설정된 변환을 적용\n",
    "        \n",
    "        return transformed  # 변환된 이미지 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a683988-0f73-4e43-907b-0d5209550abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.283239Z",
     "iopub.status.busy": "2024-09-20T15:15:26.283152Z",
     "iopub.status.idle": "2024-09-20T15:15:26.286917Z",
     "shell.execute_reply": "2024-09-20T15:15:26.286612Z"
    },
    "papermill": {
     "duration": 0.007625,
     "end_time": "2024-09-20T15:15:26.287535",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.279910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class UnsharpMask(A.ImageOnlyTransform):\n",
    "    def __init__(self, kernel_size=5, sigma=1.0, amount=1.0, threshold=0, always_apply=False, p=1.0):\n",
    "        super(UnsharpMask, self).__init__(always_apply, p)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.sigma = sigma\n",
    "        self.amount = amount\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def apply(self, image, **params):\n",
    "        return self.unsharp_mask(image)\n",
    "\n",
    "    def unsharp_mask(self, image):\n",
    "        blurred = cv2.GaussianBlur(image, (self.kernel_size, self.kernel_size), self.sigma)\n",
    "        sharpened = cv2.addWeighted(image, 1.0 + self.amount, blurred, -self.amount, 0)\n",
    "        if self.threshold > 0:\n",
    "            low_contrast_mask = np.absolute(image - blurred) < self.threshold\n",
    "            np.copyto(sharpened, image, where=low_contrast_mask)\n",
    "        return sharpened\n",
    "\n",
    "\n",
    "class AlbumentationsTransform:\n",
    "    def __init__(self, is_train: bool = True):\n",
    "        # 공통 변환 설정: 이미지 리사이즈, 정규화, 텐서 변환\n",
    "        common_transforms = [\n",
    "            A.Resize(224, 224),  # 이미지를 224x224 크기로 리사이즈\n",
    "            A.ToGray(p=1.0),  # 그레이스케일 변환\n",
    "            UnsharpMask(kernel_size=7, sigma=1.5, amount=1.5, threshold=0, p=1.0),  # 언샤프 마스크 적용 # strong\n",
    "            A.Normalize(mean=[0.5], std=[0.5]),  # 그레이스케일 이미지에 맞는 정규화\n",
    "            # A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # 정규화\n",
    "            ToTensorV2()  # albumentations에서 제공하는 PyTorch 텐서 변환\n",
    "        ]\n",
    "        \n",
    "        if is_train:\n",
    "            # 훈련용 변환: 랜덤 수평 뒤집기, 랜덤 회전, 랜덤 밝기 및 대비 조정 추가\n",
    "            self.transform = A.Compose(\n",
    "                [\n",
    "                    A.HorizontalFlip(p=0.5),  # 수평 뒤집기\n",
    "                    A.Rotate(limit=10, p=0.5),  # Rotation (회전 범위 제한)\n",
    "                    A.RandomResizedCrop(height=224, width=224, scale=(0.8, 1.0), ratio=(0.75, 1.33), p=0.3),  # 랜덤 크롭\n",
    "                    # A.RandomBrightnessContrast(brightness_limit=(-0.2, -0.2), contrast_limit=(0.2, 0.2), p=0.2),  # 밝기 및 대비 조정\n",
    "                    A.RandomBrightnessContrast(brightness_limit=(-0.2, -0.2), contrast_limit=0, p=0.9), # 90%확률로 20% 어둡게 # 전반적으로 이미지의 밝기와 채도가 높은 편이라서..!!\n",
    "                    A.OneOf([A.Emboss(p=0.3), A.Sharpen(p=0.3)], p=0.3),  # Emboss & Sharpen\n",
    "                    A.GaussianBlur(blur_limit=(3, 5), p=0.3), # 약간의 블러 추가\n",
    "                    A.CoarseDropout(max_holes=6, max_height=8, max_width=8, p=0.4), # 블럭 추가\n",
    "                    A.ElasticTransform(alpha=0.3, sigma=10, alpha_affine=5, p=0.3),  # Elastic Transform (강도 조정)       \n",
    "                    # A.GridDistortion(always_apply=False, p=1, num_steps=1, distort_limit=(-0.03, 0.05), interpolation=2, border_mode=0, value=(0, 0, 0), mask_value=None),                    \n",
    "                    # A.Affine(scale=(0.95, 1.05), shear=(-3, 3), p=0.5),  # Affine (스케일 및 쉬어 변형)                    \n",
    "                ] + common_transforms\n",
    "            )\n",
    "        else:\n",
    "            # 검증/테스트용 변환: 공통 변환만 적용\n",
    "            self.transform = A.Compose(common_transforms)\n",
    "\n",
    "    def __call__(self, image) -> torch.Tensor:\n",
    "        # 이미지가 NumPy 배열인지 확인\n",
    "        if not isinstance(image, np.ndarray):\n",
    "            raise TypeError(\"Image should be a NumPy array (OpenCV format).\")\n",
    "        \n",
    "        # 이미지에 변환 적용 및 결과 반환\n",
    "        transformed = self.transform(image=image)  # 이미지에 설정된 변환을 적용\n",
    "        \n",
    "        return transformed['image']  # 변환된 이미지의 텐서를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e82f3416-86f2-430f-9260-d23904e757e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.293854Z",
     "iopub.status.busy": "2024-09-20T15:15:26.293487Z",
     "iopub.status.idle": "2024-09-20T15:15:26.295820Z",
     "shell.execute_reply": "2024-09-20T15:15:26.295621Z"
    },
    "papermill": {
     "duration": 0.006155,
     "end_time": "2024-09-20T15:15:26.296437",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.290282",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformSelector:\n",
    "    \"\"\"\n",
    "    이미지 변환 라이브러리를 선택하기 위한 클래스.\n",
    "    \"\"\"\n",
    "    def __init__(self, transform_type: str):\n",
    "\n",
    "        # 지원하는 변환 라이브러리인지 확인\n",
    "        if transform_type in [\"torchvision\", \"albumentations\"]:\n",
    "            self.transform_type = transform_type\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unknown transformation library specified.\")\n",
    "\n",
    "    def get_transform(self, is_train: bool):\n",
    "        \n",
    "        # 선택된 라이브러리에 따라 적절한 변환 객체를 생성\n",
    "        if self.transform_type == 'torchvision':\n",
    "            transform = TorchvisionTransform(is_train=is_train)\n",
    "        \n",
    "        elif self.transform_type == 'albumentations':\n",
    "            transform = AlbumentationsTransform(is_train=is_train)\n",
    "        \n",
    "        return transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c938bcb2-9257-49cb-8d05-dd4a7bb25665",
   "metadata": {
    "papermill": {
     "duration": 0.002783,
     "end_time": "2024-09-20T15:15:26.302078",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.299295",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f16fb24a-8d34-4ed6-8a33-2d153d12d190",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.308128Z",
     "iopub.status.busy": "2024-09-20T15:15:26.307987Z",
     "iopub.status.idle": "2024-09-20T15:15:26.311252Z",
     "shell.execute_reply": "2024-09-20T15:15:26.311010Z"
    },
    "papermill": {
     "duration": 0.007464,
     "end_time": "2024-09-20T15:15:26.312314",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.304850",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    간단한 CNN 아키텍처를 정의하는 클래스.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # 순전파 함수 정의\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f91493ca-c5c2-4950-916a-cc4304c7ad4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.322648Z",
     "iopub.status.busy": "2024-09-20T15:15:26.322522Z",
     "iopub.status.idle": "2024-09-20T15:15:26.325692Z",
     "shell.execute_reply": "2024-09-20T15:15:26.325284Z"
    },
    "papermill": {
     "duration": 0.008896,
     "end_time": "2024-09-20T15:15:26.326274",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.317378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TorchvisionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Torchvision에서 제공하는 사전 훈련된 모델을 사용하는 클래스.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str, \n",
    "        num_classes: int, \n",
    "        pretrained: bool\n",
    "    ):\n",
    "        super(TorchvisionModel, self).__init__()\n",
    "        self.model = models.__dict__[model_name](pretrained=pretrained)\n",
    "        \n",
    "        # 모델의 최종 분류기 부분을 사용자 정의 클래스 수에 맞게 조정\n",
    "        if 'fc' in dir(self.model):\n",
    "            num_ftrs = self.model.fc.in_features\n",
    "            self.model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        \n",
    "        elif 'classifier' in dir(self.model):\n",
    "            num_ftrs = self.model.classifier[-1].in_features\n",
    "            self.model.classifier[-1] = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f28c8e4f-a914-4b12-982e-d4a58863c717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.332382Z",
     "iopub.status.busy": "2024-09-20T15:15:26.332238Z",
     "iopub.status.idle": "2024-09-20T15:15:26.334844Z",
     "shell.execute_reply": "2024-09-20T15:15:26.334461Z"
    },
    "papermill": {
     "duration": 0.006349,
     "end_time": "2024-09-20T15:15:26.335406",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.329057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimmModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Timm 라이브러리를 사용하여 다양한 사전 훈련된 모델을 제공하는 클래스.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_name: str, \n",
    "        num_classes: int, \n",
    "        pretrained: bool\n",
    "    ):\n",
    "        super(TimmModel, self).__init__()\n",
    "        self.model = timm.create_model(\n",
    "            model_name, \n",
    "            pretrained=pretrained, \n",
    "            num_classes=num_classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f2da081-9010-431d-a049-835d7bbea4a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.341650Z",
     "iopub.status.busy": "2024-09-20T15:15:26.341435Z",
     "iopub.status.idle": "2024-09-20T15:15:26.344343Z",
     "shell.execute_reply": "2024-09-20T15:15:26.343977Z"
    },
    "papermill": {
     "duration": 0.006699,
     "end_time": "2024-09-20T15:15:26.344902",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.338203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelSelector:\n",
    "    \"\"\"\n",
    "    사용할 모델 유형을 선택하는 클래스.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model_type: str, \n",
    "        num_classes: int, \n",
    "        **kwargs\n",
    "    ):\n",
    "        \n",
    "        # 모델 유형에 따라 적절한 모델 객체를 생성\n",
    "        if model_type == 'simple':\n",
    "            self.model = SimpleCNN(num_classes=num_classes)\n",
    "        \n",
    "        elif model_type == 'torchvision':\n",
    "            self.model = TorchvisionModel(num_classes=num_classes, **kwargs)\n",
    "        \n",
    "        elif model_type == 'timm':\n",
    "            self.model = TimmModel(num_classes=num_classes, **kwargs)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unknown model type specified.\")\n",
    "\n",
    "    def get_model(self) -> nn.Module:\n",
    "\n",
    "        # 생성된 모델 객체 반환\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2977c7b-bc39-48f7-8155-ef6b6a03d6f8",
   "metadata": {
    "papermill": {
     "duration": 0.002721,
     "end_time": "2024-09-20T15:15:26.350344",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.347623",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Loss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1b52141",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.356420Z",
     "iopub.status.busy": "2024-09-20T15:15:26.356153Z",
     "iopub.status.idle": "2024-09-20T15:15:26.359062Z",
     "shell.execute_reply": "2024-09-20T15:15:26.358679Z"
    },
    "papermill": {
     "duration": 0.006567,
     "end_time": "2024-09-20T15:15:26.359636",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.353069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes: int, smoothing: float = 0.1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "\n",
    "    def forward(self, x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        log_probs = F.log_softmax(x, dim=-1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(log_probs)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * log_probs, dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c51e6c6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.365967Z",
     "iopub.status.busy": "2024-09-20T15:15:26.365736Z",
     "iopub.status.idle": "2024-09-20T15:15:26.368555Z",
     "shell.execute_reply": "2024-09-20T15:15:26.368265Z"
    },
    "papermill": {
     "duration": 0.006843,
     "end_time": "2024-09-20T15:15:26.369257",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.362414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Cross Entropy Loss 계산\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        # 예측 확률 계산\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        # Focal Loss 계산\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(focal_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(focal_loss)\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97471eb3-a979-4fb3-b976-6c3177c79f76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.375444Z",
     "iopub.status.busy": "2024-09-20T15:15:26.375298Z",
     "iopub.status.idle": "2024-09-20T15:15:26.377420Z",
     "shell.execute_reply": "2024-09-20T15:15:26.377140Z"
    },
    "papermill": {
     "duration": 0.005862,
     "end_time": "2024-09-20T15:15:26.377988",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.372126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    \"\"\"\n",
    "    모델의 손실함수를 계산하는 클래스.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Loss, self).__init__()\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        outputs: torch.Tensor, \n",
    "        targets: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "    \n",
    "        return self.loss_fn(outputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3e3d21-5ab8-41b0-aa5c-e62ace8dc6a6",
   "metadata": {
    "papermill": {
     "duration": 0.002798,
     "end_time": "2024-09-20T15:15:26.383549",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.380751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a90c673-6672-4066-a9ec-9975d7842be4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.389603Z",
     "iopub.status.busy": "2024-09-20T15:15:26.389497Z",
     "iopub.status.idle": "2024-09-20T15:15:26.396351Z",
     "shell.execute_reply": "2024-09-20T15:15:26.396071Z"
    },
    "papermill": {
     "duration": 0.010705,
     "end_time": "2024-09-20T15:15:26.397027",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.386322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        model: nn.Module, \n",
    "        device: torch.device, \n",
    "        train_loader: DataLoader, \n",
    "        val_loader: DataLoader, \n",
    "        optimizer: optim.Optimizer,\n",
    "        scheduler: optim.lr_scheduler,\n",
    "        loss_fn: torch.nn.modules.loss._Loss, \n",
    "        epochs: int,\n",
    "        result_path: str\n",
    "    ):\n",
    "        # 클래스 초기화: 모델, 디바이스, 데이터 로더 등 설정\n",
    "        self.model = model  # 훈련할 모델\n",
    "        self.device = device  # 연산을 수행할 디바이스 (CPU or GPU)\n",
    "        self.train_loader = train_loader  # 훈련 데이터 로더\n",
    "        self.val_loader = val_loader  # 검증 데이터 로더\n",
    "        self.optimizer = optimizer  # 최적화 알고리즘\n",
    "        self.scheduler = scheduler  # 학습률 스케줄러\n",
    "        self.loss_fn = loss_fn  # 손실 함수\n",
    "        self.epochs = epochs  # 총 훈련 에폭 수\n",
    "        self.result_path = result_path  # 모델 저장 경로\n",
    "        self.best_models = []  # 가장 좋은 상위 3개 모델의 정보를 저장할 리스트\n",
    "        self.lowest_loss = float('inf')  # 가장 낮은 Loss를 저장할 변수\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        # 모델 저장 경로 설정\n",
    "        os.makedirs(self.result_path, exist_ok=True)\n",
    "\n",
    "        # 현재 에폭 모델 저장\n",
    "        current_model_path = os.path.join(self.result_path, f'model_epoch_{epoch}_loss_{loss:.4f}.pt')\n",
    "        torch.save(self.model.state_dict(), current_model_path)\n",
    "\n",
    "        # 최상위 3개 모델 관리\n",
    "        self.best_models.append((loss, epoch, current_model_path))\n",
    "        self.best_models.sort()\n",
    "        if len(self.best_models) > 3:\n",
    "            _, _, path_to_remove = self.best_models.pop(-1)  # 가장 높은 손실 모델 삭제\n",
    "            if os.path.exists(path_to_remove):\n",
    "                os.remove(path_to_remove)\n",
    "\n",
    "        # 가장 낮은 손실의 모델 저장\n",
    "        if loss < self.lowest_loss:\n",
    "            self.lowest_loss = loss\n",
    "            best_model_path = os.path.join(self.result_path, 'best_model.pt')\n",
    "            torch.save(self.model.state_dict(), best_model_path)\n",
    "            print(f\"Save {epoch}epoch result. Loss = {loss:.4f}\")\n",
    "\n",
    "    def train_epoch(self) -> float:\n",
    "        # 한 에폭 동안의 훈련을 진행\n",
    "        self.model.train()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        progress_bar = tqdm(self.train_loader, desc=\"Training\", leave=False)\n",
    "\n",
    "        for images, targets in progress_bar:\n",
    "            images, targets = images.to(self.device), targets.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(images)\n",
    "            loss = self.loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        return total_loss / len(self.train_loader)\n",
    "\n",
    "    def validate(self) -> float:\n",
    "        # 모델의 검증을 진행\n",
    "        self.model.eval()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        progress_bar = tqdm(self.val_loader, desc=\"Validating\", leave=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, targets in progress_bar:\n",
    "                images, targets = images.to(self.device), targets.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                loss = self.loss_fn(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        return total_loss / len(self.val_loader)\n",
    "\n",
    "    def train(self) -> float:\n",
    "        # 전체 훈련 과정을 관리\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "\n",
    "            train_loss = self.train_epoch()\n",
    "            val_loss = self.validate()\n",
    "\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Learning Rate: {current_lr:.6f}\\n\")\n",
    "\n",
    "            self.save_model(epoch, val_loss)\n",
    "            self.scheduler.step()\n",
    "\n",
    "        # 학습 완료 후 최종 검증 손실 반환\n",
    "        return self.lowest_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f41c09-318a-4f2e-bdca-68d8a07e9938",
   "metadata": {
    "papermill": {
     "duration": 0.002798,
     "end_time": "2024-09-20T15:15:26.402940",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.400142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "698783c4-ac2a-4e66-82aa-637df06ce012",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.409207Z",
     "iopub.status.busy": "2024-09-20T15:15:26.409018Z",
     "iopub.status.idle": "2024-09-20T15:15:26.410877Z",
     "shell.execute_reply": "2024-09-20T15:15:26.410616Z"
    },
    "papermill": {
     "duration": 0.005818,
     "end_time": "2024-09-20T15:15:26.411529",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.405711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 학습에 사용할 장비를 선택.\n",
    "# torch라이브러리에서 gpu를 인식할 경우, cuda로 설정.\n",
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76cfe17e-fb14-42e4-84ae-b6773f0b78fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.417567Z",
     "iopub.status.busy": "2024-09-20T15:15:26.417423Z",
     "iopub.status.idle": "2024-09-20T15:15:26.419057Z",
     "shell.execute_reply": "2024-09-20T15:15:26.418823Z"
    },
    "papermill": {
     "duration": 0.005354,
     "end_time": "2024-09-20T15:15:26.419662",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.414308",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 학습 데이터의 경로와 정보를 가진 파일의 경로를 설정.\n",
    "traindata_dir = \"./data/train\"\n",
    "traindata_info_file = \"./data/train.csv\"\n",
    "save_result_path = \"./train_result_code8_nf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa914f90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.425829Z",
     "iopub.status.busy": "2024-09-20T15:15:26.425623Z",
     "iopub.status.idle": "2024-09-20T15:15:26.434785Z",
     "shell.execute_reply": "2024-09-20T15:15:26.434522Z"
    },
    "papermill": {
     "duration": 0.012896,
     "end_time": "2024-09-20T15:15:26.435380",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.422484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 학습 데이터의 class, image path, target에 대한 정보가 들어있는 csv파일을 읽기.\n",
    "train_info = pd.read_csv(traindata_info_file)\n",
    "\n",
    "# 총 class의 수를 측정.\n",
    "num_classes = len(train_info['target'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db231faf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.441484Z",
     "iopub.status.busy": "2024-09-20T15:15:26.441347Z",
     "iopub.status.idle": "2024-09-20T15:15:26.442969Z",
     "shell.execute_reply": "2024-09-20T15:15:26.442735Z"
    },
    "papermill": {
     "duration": 0.005314,
     "end_time": "2024-09-20T15:15:26.443530",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.438216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KFold 설정\n",
    "n_splits = 5  # 5-Fold Cross Validation\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfcf3912",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.449619Z",
     "iopub.status.busy": "2024-09-20T15:15:26.449353Z",
     "iopub.status.idle": "2024-09-20T15:15:26.452711Z",
     "shell.execute_reply": "2024-09-20T15:15:26.452312Z"
    },
    "papermill": {
     "duration": 0.007138,
     "end_time": "2024-09-20T15:15:26.453421",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.446283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 학습에 사용할 Transform을 선언.\n",
    "transform_selector = TransformSelector(\n",
    "    transform_type = \"albumentations\"\n",
    ")\n",
    "train_transform = transform_selector.get_transform(is_train=True)\n",
    "val_transform = transform_selector.get_transform(is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b283513",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.460376Z",
     "iopub.status.busy": "2024-09-20T15:15:26.459888Z",
     "iopub.status.idle": "2024-09-20T15:15:26.462216Z",
     "shell.execute_reply": "2024-09-20T15:15:26.461878Z"
    },
    "papermill": {
     "duration": 0.006594,
     "end_time": "2024-09-20T15:15:26.462916",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.456322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fold_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32715cca-5a4a-49d5-8fd9-f84da4581523",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.469341Z",
     "iopub.status.busy": "2024-09-20T15:15:26.468971Z",
     "iopub.status.idle": "2024-09-20T15:15:26.471057Z",
     "shell.execute_reply": "2024-09-20T15:15:26.470700Z"
    },
    "papermill": {
     "duration": 0.006012,
     "end_time": "2024-09-20T15:15:26.471722",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.465710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 학습에 사용할 Loss를 선언.\n",
    "# loss_fn = Loss() #cross_entropy_loss\n",
    "loss_fn = FocalLoss(alpha=1, gamma=2) #focal_loss\n",
    "# loss_fn = LabelSmoothingLoss(classes=num_classes, smoothing=0.1) #Label_smoothing_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10011e8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.477901Z",
     "iopub.status.busy": "2024-09-20T15:15:26.477619Z",
     "iopub.status.idle": "2024-09-20T15:15:26.479230Z",
     "shell.execute_reply": "2024-09-20T15:15:26.478993Z"
    },
    "papermill": {
     "duration": 0.005205,
     "end_time": "2024-09-20T15:15:26.479704",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.474499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 스케줄러 초기화\n",
    "# scheduler_step_size = 15  # 매 15step마다 학습률 감소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ac64a4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-20T15:15:26.485795Z",
     "iopub.status.busy": "2024-09-20T15:15:26.485706Z",
     "iopub.status.idle": "2024-09-21T13:13:07.029543Z",
     "shell.execute_reply": "2024-09-21T13:13:07.029192Z"
    },
    "papermill": {
     "duration": 79060.547922,
     "end_time": "2024-09-21T13:13:07.030422",
     "exception": false,
     "start_time": "2024-09-20T15:15:26.482500",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<1>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 82\u001b[0m\n\u001b[1;32m     69\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     70\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[1;32m     71\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m         result_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_result_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/fold_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# 각 fold 결과 저장\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     )\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# 모델 학습 및 결과 저장\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m     fold_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     fold_results\u001b[38;5;241m.\u001b[39mappend(fold_result)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# 각 Fold의 결과를 기반으로 평균 성능 계산\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 92\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 92\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidate()\n\u001b[1;32m     95\u001b[0m     current_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[19], line 57\u001b[0m, in \u001b[0;36mTrainer.train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     55\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[1;32m     58\u001b[0m     images, targets \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), targets\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/conda/envs/blueberry/lib/python3.9/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/blueberry/lib/python3.9/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/envs/blueberry/lib/python3.9/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/envs/blueberry/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/envs/blueberry/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[8], line 34\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# BPD와 MSR을 적용하는 부분 추가\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_bpd_msr:\n\u001b[0;32m---> 34\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mapply_bpd_with_partial_msr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcluster_centers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmsr_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# MSR을 50%만 적용\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_deformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     image \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(image)  \u001b[38;5;66;03m# 변형된 이미지 중 하나 선택\u001b[39;00m\n\u001b[1;32m     43\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)  \u001b[38;5;66;03m# 설정된 이미지 변환을 적용합니다.# Albumentations 또는 기타 transform 적용\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m, in \u001b[0;36mapply_bpd_with_partial_msr\u001b[0;34m(image, cluster_centers, msr_ratio, num_deformations)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m deformed_images:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# NumPy 배열 비교 수정\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(np\u001b[38;5;241m.\u001b[39marray_equal(img, msr_img) \u001b[38;5;28;01mfor\u001b[39;00m msr_img \u001b[38;5;129;01min\u001b[39;00m msr_images):\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# MSR 적용\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mmean_stroke_reconstruction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcluster_centers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     final_images\u001b[38;5;241m.\u001b[39mappend(img)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m final_images\n",
      "Cell \u001b[0;32mIn[6], line 76\u001b[0m, in \u001b[0;36mmean_stroke_reconstruction\u001b[0;34m(image, cluster_centers, patch_size)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean_stroke_reconstruction\u001b[39m(image, cluster_centers, patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m31\u001b[39m):\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m    계산된 클러스터 중심을 사용하여 입력 이미지에 Mean Stroke Reconstruction을 적용하는 함수.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    - Reconstructed image (numpy.ndarray): 재구성된 이미지.\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     patches, hog_features \u001b[38;5;241m=\u001b[39m \u001b[43mextract_patches_and_hog_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# 각 패치를 클러스터 중심으로 대체\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(cluster_centers))\u001b[38;5;241m.\u001b[39mfit(hog_features)\n",
      "Cell \u001b[0;32mIn[6], line 19\u001b[0m, in \u001b[0;36mextract_patches_and_hog_features\u001b[0;34m(image, patch_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_patches_and_hog_features\u001b[39m(image, patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m31\u001b[39m):\n\u001b[1;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    주어진 이미지에서 패치를 추출하고, 각 패치의 HOG 특징을 계산하는 함수.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    - hog_features (List): HOG 특징의 리스트.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     gray \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_RGB2GRAY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     _, binary \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mthreshold(gray, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m255\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mTHRESH_BINARY)\n\u001b[1;32m     21\u001b[0m     skeleton \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mximgproc\u001b[38;5;241m.\u001b[39mthinning(binary)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:92: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<1>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Invalid number of channels in input image:\n>     'VScn::contains(scn)'\n> where\n>     'scn' is 1\n"
     ]
    }
   ],
   "source": [
    "# KFold 교차 검증 수행\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_info, train_info['target'])):\n",
    "    print(f'Fold {fold + 1}/{n_splits}')\n",
    "\n",
    "    # train_df와 val_df를 train_idx와 val_idx로 분할\n",
    "    train_df = train_info.iloc[train_idx]\n",
    "    val_df = train_info.iloc[val_idx]\n",
    "    \n",
    "    # 클러스터 중심 계산 (train_df의 이미지 사용)\n",
    "    train_images = [cv2.imread(os.path.join(traindata_dir, path), cv2.IMREAD_COLOR) for path in train_df['image_path'].tolist()]\n",
    "    cluster_centers = compute_cluster_centers(train_images, n_clusters=150, patch_size=31)\n",
    "\n",
    "\n",
    "    # 학습에 사용할 Model 선언 (매 Fold마다 모델을 초기화)\n",
    "    model_selector = ModelSelector(\n",
    "        model_type='timm', \n",
    "        num_classes=num_classes,\n",
    "        model_name='dm_nfnet_f0', \n",
    "        pretrained=True\n",
    "    )\n",
    "    model = model_selector.get_model().to(device)\n",
    "\n",
    "    # optimizer 선언\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001) # lr=0.0001은 100 epoch일때..\n",
    "\n",
    "    # 학습에 사용할 Dataset 선언\n",
    "    train_dataset = CustomDataset(\n",
    "        root_dir=traindata_dir,\n",
    "        info_df=train_df,\n",
    "        transform=train_transform,\n",
    "        cluster_centers=cluster_centers,  # MSR에 사용할 클러스터 중심\n",
    "        apply_bpd_msr=True  # BPD와 MSR 적용 활성화\n",
    "    )\n",
    "    val_dataset = CustomDataset(\n",
    "        root_dir=traindata_dir,\n",
    "        info_df=val_df,\n",
    "        transform=val_transform,\n",
    "        cluster_centers=None,  # Validation에서는 MSR을 적용하지 않음\n",
    "        apply_bpd_msr=False  # BPD와 MSR 적용하지 않음\n",
    "    )\n",
    "\n",
    "    # 학습에 사용할 DataLoader 선언\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    # 한 epoch당 step 수 계산\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    \n",
    "    scheduler_gamma = 0.7  # 학습률을 현재의 70%로 감소 # 0.001 -> 0.0007 -> ..\n",
    "    epochs = 10\n",
    "    # StepLR\n",
    "    # 15 epoch마다 학습률을 감소시키는 스케줄러 선언\n",
    "    # 5 epoch마다 학습률을 감소시키는 스케줄러 선언\n",
    "    # decay 25로 해서 lr 안 줄어들게 일단 ㄱ\n",
    "    epochs_per_lr_decay = 25 # 100 epoch에서 decay 25로 설정했었음\n",
    "    scheduler_step_size = steps_per_epoch * epochs_per_lr_decay\n",
    "    \n",
    "    # 스케줄러 선언\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "        optimizer, \n",
    "        step_size=scheduler_step_size, \n",
    "        gamma=scheduler_gamma\n",
    "    )\n",
    "    \n",
    "    # #CosineAnnealingLR\n",
    "    # scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=steps_per_epoch * epochs)\n",
    "\n",
    "    # Trainer 선언\n",
    "    trainer = Trainer(\n",
    "        model=model, \n",
    "        device=device, \n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader, \n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        loss_fn=loss_fn, \n",
    "        epochs=epochs,  # 각 fold마다 동일한 epoch수로 학습\n",
    "        result_path=f\"{save_result_path}/fold_{fold + 1}\"  # 각 fold 결과 저장\n",
    "    )\n",
    "\n",
    "    # 모델 학습 및 결과 저장\n",
    "    fold_result = trainer.train()\n",
    "    fold_results.append(fold_result)\n",
    "\n",
    "# 각 Fold의 결과를 기반으로 평균 성능 계산\n",
    "average_performance = sum(fold_results) / len(fold_results)\n",
    "print(f'KFold 평균 성능: {average_performance}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11087088-9b1f-4f7d-8eb5-72008cc88a50",
   "metadata": {
    "papermill": {
     "duration": 2.627969,
     "end_time": "2024-09-21T13:13:12.051967",
     "exception": false,
     "start_time": "2024-09-21T13:13:09.423998",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64901ddc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:13:17.083672Z",
     "iopub.status.busy": "2024-09-21T13:13:17.083510Z",
     "iopub.status.idle": "2024-09-21T13:13:17.085983Z",
     "shell.execute_reply": "2024-09-21T13:13:17.085635Z"
    },
    "papermill": {
     "duration": 2.530779,
     "end_time": "2024-09-21T13:13:17.086631",
     "exception": false,
     "start_time": "2024-09-21T13:13:14.555852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 폴드 수 및 모델 저장 경로 설정\n",
    "n_folds = 5\n",
    "fold_model_paths = [f\"./train_result_code8_nf/fold_{fold + 1}/best_model.pt\" for fold in range(n_folds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2087a35c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:13:22.115465Z",
     "iopub.status.busy": "2024-09-21T13:13:22.115273Z",
     "iopub.status.idle": "2024-09-21T13:13:22.117939Z",
     "shell.execute_reply": "2024-09-21T13:13:22.117608Z"
    },
    "papermill": {
     "duration": 2.508161,
     "end_time": "2024-09-21T13:13:22.118663",
     "exception": false,
     "start_time": "2024-09-21T13:13:19.610502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./train_result_code8_nf/fold_1/best_model.pt', './train_result_code8_nf/fold_2/best_model.pt', './train_result_code8_nf/fold_3/best_model.pt', './train_result_code8_nf/fold_4/best_model.pt', './train_result_code8_nf/fold_5/best_model.pt']\n"
     ]
    }
   ],
   "source": [
    "print(fold_model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59e016d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:13:27.133627Z",
     "iopub.status.busy": "2024-09-21T13:13:27.133439Z",
     "iopub.status.idle": "2024-09-21T13:13:27.137258Z",
     "shell.execute_reply": "2024-09-21T13:13:27.136916Z"
    },
    "papermill": {
     "duration": 2.6382,
     "end_time": "2024-09-21T13:13:27.137912",
     "exception": false,
     "start_time": "2024-09-21T13:13:24.499712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 저장된 모델을 불러와서 앙상블을 수행하는 함수\n",
    "def ensemble_predict_folds(\n",
    "    fold_model_paths: list, \n",
    "    device: torch.device, \n",
    "    test_loader: DataLoader\n",
    "    ):\n",
    "    models = []\n",
    "    \n",
    "    # 각 폴드의 베스트 모델 불러오기\n",
    "    for fold_path in fold_model_paths:\n",
    "        # 모델 초기화 및 로드\n",
    "        model = ModelSelector(\n",
    "            model_type='timm', \n",
    "            num_classes=num_classes,\n",
    "            model_name='dm_nfnet_f0', \n",
    "            pretrained=False\n",
    "        ).get_model().to(device)\n",
    "        model.load_state_dict(torch.load(fold_path, map_location=device))\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "    \n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(test_loader):\n",
    "            # 이미지를 GPU 또는 CPU로 이동\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # 폴드별 예측 수행\n",
    "            fold_preds = []\n",
    "            for model in models:\n",
    "                logits = model(images)\n",
    "                logits = F.softmax(logits, dim=1)  # 확률값으로 변환\n",
    "                fold_preds.append(logits)\n",
    "            \n",
    "            # 폴드별 예측 결과 평균\n",
    "            avg_preds = torch.mean(torch.stack(fold_preds), dim=0)\n",
    "            final_preds = avg_preds.argmax(dim=1)\n",
    "            \n",
    "            # 예측 결과 저장\n",
    "            predictions.extend(final_preds.cpu().detach().numpy())\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b407c24c-785d-4ffc-b17b-84ae7dc4ecae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:13:32.178389Z",
     "iopub.status.busy": "2024-09-21T13:13:32.178203Z",
     "iopub.status.idle": "2024-09-21T13:13:32.180684Z",
     "shell.execute_reply": "2024-09-21T13:13:32.180261Z"
    },
    "papermill": {
     "duration": 2.506745,
     "end_time": "2024-09-21T13:13:32.181385",
     "exception": false,
     "start_time": "2024-09-21T13:13:29.674640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 추론 데이터의 경로와 정보를 가진 파일의 경로를 설정.\n",
    "testdata_dir = \"./data/test\"\n",
    "testdata_info_file = \"./data/test.csv\"\n",
    "save_result_path = \"./train_result_code8_nf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbb89c12-3b5d-4647-a8c2-83650dce6281",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:13:37.055772Z",
     "iopub.status.busy": "2024-09-21T13:13:37.055589Z",
     "iopub.status.idle": "2024-09-21T13:13:37.060111Z",
     "shell.execute_reply": "2024-09-21T13:13:37.059772Z"
    },
    "papermill": {
     "duration": 2.354958,
     "end_time": "2024-09-21T13:13:37.060798",
     "exception": false,
     "start_time": "2024-09-21T13:13:34.705840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 추론 데이터의 class, image path, target에 대한 정보가 들어있는 csv파일을 읽기.\n",
    "test_info = pd.read_csv(testdata_info_file)\n",
    "\n",
    "# 총 class 수.\n",
    "num_classes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecec8773-6045-401e-b307-0a9758374c4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:13:42.223291Z",
     "iopub.status.busy": "2024-09-21T13:13:42.223105Z",
     "iopub.status.idle": "2024-09-21T13:13:42.226506Z",
     "shell.execute_reply": "2024-09-21T13:13:42.226159Z"
    },
    "papermill": {
     "duration": 2.530057,
     "end_time": "2024-09-21T13:13:42.227192",
     "exception": false,
     "start_time": "2024-09-21T13:13:39.697135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 추론에 사용할 Transform을 선언.\n",
    "transform_selector = TransformSelector(\n",
    "    transform_type = \"albumentations\"\n",
    ")\n",
    "test_transform = transform_selector.get_transform(is_train=False)\n",
    "\n",
    "# 추론에 사용할 Dataset을 선언.\n",
    "test_dataset = CustomDataset(\n",
    "    root_dir=testdata_dir,\n",
    "    info_df=test_info,\n",
    "    transform=test_transform,\n",
    "    is_inference=True\n",
    ")\n",
    "\n",
    "# 추론에 사용할 DataLoader를 선언.\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bb15fde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:13:47.253629Z",
     "iopub.status.busy": "2024-09-21T13:13:47.253385Z",
     "iopub.status.idle": "2024-09-21T13:14:55.732302Z",
     "shell.execute_reply": "2024-09-21T13:14:55.731809Z"
    },
    "papermill": {
     "duration": 70.986961,
     "end_time": "2024-09-21T13:14:55.733195",
     "exception": false,
     "start_time": "2024-09-21T13:13:44.746234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  0%|                                                                                                              | 0/157 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  1%|▋                                                                                                     | 1/157 [00:00<01:04,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  1%|█▎                                                                                                    | 2/157 [00:00<01:04,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  2%|█▉                                                                                                    | 3/157 [00:01<01:04,  2.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  3%|██▌                                                                                                   | 4/157 [00:01<01:04,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  3%|███▏                                                                                                  | 5/157 [00:02<01:03,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  4%|███▉                                                                                                  | 6/157 [00:02<01:03,  2.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  4%|████▌                                                                                                 | 7/157 [00:02<01:02,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  5%|█████▏                                                                                                | 8/157 [00:03<01:02,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  6%|█████▊                                                                                                | 9/157 [00:03<01:01,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  6%|██████▍                                                                                              | 10/157 [00:04<01:01,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  7%|███████                                                                                              | 11/157 [00:04<01:01,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  8%|███████▋                                                                                             | 12/157 [00:05<01:00,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  8%|████████▎                                                                                            | 13/157 [00:05<01:00,  2.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  9%|█████████                                                                                            | 14/157 [00:05<01:00,  2.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 10%|█████████▋                                                                                           | 15/157 [00:06<00:59,  2.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 10%|██████████▎                                                                                          | 16/157 [00:06<00:58,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 11%|██████████▉                                                                                          | 17/157 [00:07<00:58,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 11%|███████████▌                                                                                         | 18/157 [00:07<00:58,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 12%|████████████▏                                                                                        | 19/157 [00:07<00:57,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 13%|████████████▊                                                                                        | 20/157 [00:08<00:56,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 13%|█████████████▌                                                                                       | 21/157 [00:08<00:56,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 14%|██████████████▏                                                                                      | 22/157 [00:09<00:56,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 15%|██████████████▊                                                                                      | 23/157 [00:09<00:55,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 15%|███████████████▍                                                                                     | 24/157 [00:10<00:55,  2.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 16%|████████████████                                                                                     | 25/157 [00:10<00:55,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 17%|████████████████▋                                                                                    | 26/157 [00:11<01:02,  2.10it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 17%|█████████████████▎                                                                                   | 27/157 [00:11<00:59,  2.19it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 18%|██████████████████                                                                                   | 28/157 [00:11<00:57,  2.25it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 18%|██████████████████▋                                                                                  | 29/157 [00:12<00:56,  2.27it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 19%|███████████████████▎                                                                                 | 30/157 [00:12<00:55,  2.30it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 20%|███████████████████▉                                                                                 | 31/157 [00:13<00:53,  2.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 20%|████████████████████▌                                                                                | 32/157 [00:13<00:53,  2.35it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 21%|█████████████████████▏                                                                               | 33/157 [00:13<00:52,  2.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 22%|█████████████████████▊                                                                               | 34/157 [00:14<00:52,  2.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 22%|██████████████████████▌                                                                              | 35/157 [00:14<00:51,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 23%|███████████████████████▏                                                                             | 36/157 [00:15<00:50,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 24%|███████████████████████▊                                                                             | 37/157 [00:15<00:50,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 24%|████████████████████████▍                                                                            | 38/157 [00:16<00:49,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 25%|█████████████████████████                                                                            | 39/157 [00:16<00:49,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 25%|█████████████████████████▋                                                                           | 40/157 [00:16<00:48,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 26%|██████████████████████████▍                                                                          | 41/157 [00:17<00:48,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 27%|███████████████████████████                                                                          | 42/157 [00:17<00:47,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 27%|███████████████████████████▋                                                                         | 43/157 [00:18<00:47,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 28%|████████████████████████████▎                                                                        | 44/157 [00:18<00:47,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 29%|████████████████████████████▉                                                                        | 45/157 [00:18<00:46,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 29%|█████████████████████████████▌                                                                       | 46/157 [00:19<00:45,  2.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 30%|██████████████████████████████▏                                                                      | 47/157 [00:19<00:45,  2.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 31%|██████████████████████████████▉                                                                      | 48/157 [00:20<00:45,  2.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 31%|███████████████████████████████▌                                                                     | 49/157 [00:20<00:44,  2.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 32%|████████████████████████████████▏                                                                    | 50/157 [00:21<00:44,  2.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 32%|████████████████████████████████▊                                                                    | 51/157 [00:21<00:43,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 33%|█████████████████████████████████▍                                                                   | 52/157 [00:21<00:43,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 34%|██████████████████████████████████                                                                   | 53/157 [00:22<00:43,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 34%|██████████████████████████████████▋                                                                  | 54/157 [00:22<00:43,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 35%|███████████████████████████████████▍                                                                 | 55/157 [00:23<00:42,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 36%|████████████████████████████████████                                                                 | 56/157 [00:23<00:42,  2.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 36%|████████████████████████████████████▋                                                                | 57/157 [00:23<00:42,  2.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 37%|█████████████████████████████████████▎                                                               | 58/157 [00:24<00:41,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 38%|█████████████████████████████████████▉                                                               | 59/157 [00:24<00:40,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 38%|██████████████████████████████████████▌                                                              | 60/157 [00:25<00:40,  2.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 39%|███████████████████████████████████████▏                                                             | 61/157 [00:25<00:39,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 39%|███████████████████████████████████████▉                                                             | 62/157 [00:26<00:39,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 40%|████████████████████████████████████████▌                                                            | 63/157 [00:26<00:39,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 41%|█████████████████████████████████████████▏                                                           | 64/157 [00:26<00:38,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 41%|█████████████████████████████████████████▊                                                           | 65/157 [00:27<00:38,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 42%|██████████████████████████████████████████▍                                                          | 66/157 [00:27<00:38,  2.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 43%|███████████████████████████████████████████                                                          | 67/157 [00:28<00:37,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 43%|███████████████████████████████████████████▋                                                         | 68/157 [00:28<00:37,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 44%|████████████████████████████████████████████▍                                                        | 69/157 [00:28<00:36,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 45%|█████████████████████████████████████████████                                                        | 70/157 [00:29<00:36,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 45%|█████████████████████████████████████████████▋                                                       | 71/157 [00:29<00:35,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 46%|██████████████████████████████████████████████▎                                                      | 72/157 [00:30<00:35,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 46%|██████████████████████████████████████████████▉                                                      | 73/157 [00:30<00:34,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 47%|███████████████████████████████████████████████▌                                                     | 74/157 [00:31<00:34,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 48%|████████████████████████████████████████████████▏                                                    | 75/157 [00:31<00:34,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 48%|████████████████████████████████████████████████▉                                                    | 76/157 [00:31<00:33,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 49%|█████████████████████████████████████████████████▌                                                   | 77/157 [00:32<00:33,  2.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 50%|██████████████████████████████████████████████████▏                                                  | 78/157 [00:32<00:32,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 50%|██████████████████████████████████████████████████▊                                                  | 79/157 [00:33<00:32,  2.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 51%|███████████████████████████████████████████████████▍                                                 | 80/157 [00:33<00:31,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 52%|████████████████████████████████████████████████████                                                 | 81/157 [00:33<00:31,  2.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 52%|████████████████████████████████████████████████████▊                                                | 82/157 [00:34<00:31,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 53%|█████████████████████████████████████████████████████▍                                               | 83/157 [00:34<00:30,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 54%|██████████████████████████████████████████████████████                                               | 84/157 [00:35<00:30,  2.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 54%|██████████████████████████████████████████████████████▋                                              | 85/157 [00:35<00:29,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 55%|███████████████████████████████████████████████████████▎                                             | 86/157 [00:36<00:29,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 55%|███████████████████████████████████████████████████████▉                                             | 87/157 [00:36<00:29,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 56%|████████████████████████████████████████████████████████▌                                            | 88/157 [00:36<00:28,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 57%|█████████████████████████████████████████████████████████▎                                           | 89/157 [00:37<00:28,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 57%|█████████████████████████████████████████████████████████▉                                           | 90/157 [00:37<00:27,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 58%|██████████████████████████████████████████████████████████▌                                          | 91/157 [00:38<00:27,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 59%|███████████████████████████████████████████████████████████▏                                         | 92/157 [00:38<00:26,  2.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 59%|███████████████████████████████████████████████████████████▊                                         | 93/157 [00:38<00:26,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 60%|████████████████████████████████████████████████████████████▍                                        | 94/157 [00:39<00:26,  2.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 61%|█████████████████████████████████████████████████████████████                                        | 95/157 [00:39<00:25,  2.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 61%|█████████████████████████████████████████████████████████████▊                                       | 96/157 [00:40<00:25,  2.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 62%|██████████████████████████████████████████████████████████████▍                                      | 97/157 [00:40<00:24,  2.43it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 62%|███████████████████████████████████████████████████████████████                                      | 98/157 [00:41<00:24,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 63%|███████████████████████████████████████████████████████████████▋                                     | 99/157 [00:41<00:24,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 64%|███████████████████████████████████████████████████████████████▋                                    | 100/157 [00:41<00:23,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 64%|████████████████████████████████████████████████████████████████▎                                   | 101/157 [00:42<00:23,  2.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 65%|████████████████████████████████████████████████████████████████▉                                   | 102/157 [00:42<00:22,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 66%|█████████████████████████████████████████████████████████████████▌                                  | 103/157 [00:43<00:22,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 66%|██████████████████████████████████████████████████████████████████▏                                 | 104/157 [00:43<00:22,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 67%|██████████████████████████████████████████████████████████████████▉                                 | 105/157 [00:43<00:21,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 68%|███████████████████████████████████████████████████████████████████▌                                | 106/157 [00:44<00:21,  2.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 68%|████████████████████████████████████████████████████████████████████▏                               | 107/157 [00:44<00:20,  2.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 69%|████████████████████████████████████████████████████████████████████▊                               | 108/157 [00:45<00:20,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 69%|█████████████████████████████████████████████████████████████████████▍                              | 109/157 [00:45<00:19,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 70%|██████████████████████████████████████████████████████████████████████                              | 110/157 [00:45<00:19,  2.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 71%|██████████████████████████████████████████████████████████████████████▋                             | 111/157 [00:46<00:19,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 71%|███████████████████████████████████████████████████████████████████████▎                            | 112/157 [00:46<00:18,  2.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 72%|███████████████████████████████████████████████████████████████████████▉                            | 113/157 [00:47<00:18,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 73%|████████████████████████████████████████████████████████████████████████▌                           | 114/157 [00:47<00:17,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 73%|█████████████████████████████████████████████████████████████████████████▏                          | 115/157 [00:48<00:17,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 74%|█████████████████████████████████████████████████████████████████████████▉                          | 116/157 [00:48<00:17,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 75%|██████████████████████████████████████████████████████████████████████████▌                         | 117/157 [00:48<00:16,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 75%|███████████████████████████████████████████████████████████████████████████▏                        | 118/157 [00:49<00:16,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 76%|███████████████████████████████████████████████████████████████████████████▊                        | 119/157 [00:49<00:15,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 76%|████████████████████████████████████████████████████████████████████████████▍                       | 120/157 [00:50<00:15,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 77%|█████████████████████████████████████████████████████████████████████████████                       | 121/157 [00:50<00:14,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 78%|█████████████████████████████████████████████████████████████████████████████▋                      | 122/157 [00:51<00:14,  2.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 78%|██████████████████████████████████████████████████████████████████████████████▎                     | 123/157 [00:51<00:14,  2.37it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 79%|██████████████████████████████████████████████████████████████████████████████▉                     | 124/157 [00:51<00:13,  2.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 80%|███████████████████████████████████████████████████████████████████████████████▌                    | 125/157 [00:52<00:13,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 80%|████████████████████████████████████████████████████████████████████████████████▎                   | 126/157 [00:52<00:12,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 81%|████████████████████████████████████████████████████████████████████████████████▉                   | 127/157 [00:53<00:12,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 82%|█████████████████████████████████████████████████████████████████████████████████▌                  | 128/157 [00:53<00:12,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 82%|██████████████████████████████████████████████████████████████████████████████████▏                 | 129/157 [00:53<00:11,  2.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 83%|██████████████████████████████████████████████████████████████████████████████████▊                 | 130/157 [00:54<00:11,  2.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 83%|███████████████████████████████████████████████████████████████████████████████████▍                | 131/157 [00:54<00:10,  2.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 84%|████████████████████████████████████████████████████████████████████████████████████                | 132/157 [00:55<00:10,  2.38it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 85%|████████████████████████████████████████████████████████████████████████████████████▋               | 133/157 [00:55<00:10,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 85%|█████████████████████████████████████████████████████████████████████████████████████▎              | 134/157 [00:56<00:09,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 86%|█████████████████████████████████████████████████████████████████████████████████████▉              | 135/157 [00:56<00:09,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 87%|██████████████████████████████████████████████████████████████████████████████████████▌             | 136/157 [00:56<00:08,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 87%|███████████████████████████████████████████████████████████████████████████████████████▎            | 137/157 [00:57<00:08,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 88%|███████████████████████████████████████████████████████████████████████████████████████▉            | 138/157 [00:57<00:07,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 89%|████████████████████████████████████████████████████████████████████████████████████████▌           | 139/157 [00:58<00:07,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 89%|█████████████████████████████████████████████████████████████████████████████████████████▏          | 140/157 [00:58<00:07,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 90%|█████████████████████████████████████████████████████████████████████████████████████████▊          | 141/157 [00:58<00:06,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████▍         | 142/157 [00:59<00:06,  2.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 91%|███████████████████████████████████████████████████████████████████████████████████████████         | 143/157 [00:59<00:05,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 92%|███████████████████████████████████████████████████████████████████████████████████████████▋        | 144/157 [01:00<00:05,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 92%|████████████████████████████████████████████████████████████████████████████████████████████▎       | 145/157 [01:00<00:04,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 93%|████████████████████████████████████████████████████████████████████████████████████████████▉       | 146/157 [01:01<00:04,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 94%|█████████████████████████████████████████████████████████████████████████████████████████████▋      | 147/157 [01:01<00:04,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 94%|██████████████████████████████████████████████████████████████████████████████████████████████▎     | 148/157 [01:01<00:03,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 95%|██████████████████████████████████████████████████████████████████████████████████████████████▉     | 149/157 [01:02<00:03,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 96%|███████████████████████████████████████████████████████████████████████████████████████████████▌    | 150/157 [01:02<00:02,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 96%|████████████████████████████████████████████████████████████████████████████████████████████████▏   | 151/157 [01:03<00:02,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 97%|████████████████████████████████████████████████████████████████████████████████████████████████▊   | 152/157 [01:03<00:02,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 97%|█████████████████████████████████████████████████████████████████████████████████████████████████▍  | 153/157 [01:03<00:01,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 98%|██████████████████████████████████████████████████████████████████████████████████████████████████  | 154/157 [01:04<00:01,  2.39it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 99%|██████████████████████████████████████████████████████████████████████████████████████████████████▋ | 155/157 [01:04<00:00,  2.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " 99%|███████████████████████████████████████████████████████████████████████████████████████████████████▎| 156/157 [01:05<00:00,  2.42it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [01:05<00:00,  2.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 157/157 [01:05<00:00,  2.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 폴드별 저장된 모델을 사용한 앙상블 추론 실행\n",
    "predictions = ensemble_predict_folds(\n",
    "    fold_model_paths=fold_model_paths, \n",
    "    device=device, \n",
    "    test_loader=test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc96c889-2423-42b2-8c3c-4b1d364ece71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:15:00.872525Z",
     "iopub.status.busy": "2024-09-21T13:15:00.872339Z",
     "iopub.status.idle": "2024-09-21T13:15:00.886810Z",
     "shell.execute_reply": "2024-09-21T13:15:00.886547Z"
    },
    "papermill": {
     "duration": 2.524127,
     "end_time": "2024-09-21T13:15:00.887769",
     "exception": false,
     "start_time": "2024-09-21T13:14:58.363642",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>image_path</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.JPEG</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.JPEG</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.JPEG</td>\n",
       "      <td>493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3.JPEG</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.JPEG</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>10009</td>\n",
       "      <td>10009.JPEG</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10010</th>\n",
       "      <td>10010</td>\n",
       "      <td>10010.JPEG</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10011</th>\n",
       "      <td>10011</td>\n",
       "      <td>10011.JPEG</td>\n",
       "      <td>466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10012</th>\n",
       "      <td>10012</td>\n",
       "      <td>10012.JPEG</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10013</th>\n",
       "      <td>10013</td>\n",
       "      <td>10013.JPEG</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10014 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID  image_path  target\n",
       "0          0      0.JPEG     328\n",
       "1          1      1.JPEG     414\n",
       "2          2      2.JPEG     493\n",
       "3          3      3.JPEG      17\n",
       "4          4      4.JPEG     388\n",
       "...      ...         ...     ...\n",
       "10009  10009  10009.JPEG     235\n",
       "10010  10010  10010.JPEG     191\n",
       "10011  10011  10011.JPEG     466\n",
       "10012  10012  10012.JPEG     400\n",
       "10013  10013  10013.JPEG     210\n",
       "\n",
       "[10014 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 클래스에 대한 예측 결과를 하나의 문자열로 합침\n",
    "test_info['target'] = predictions\n",
    "test_info = test_info.reset_index().rename(columns={\"index\": \"ID\"})\n",
    "test_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4efd2f6-d74a-491b-a7b1-fd7cf96f45a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:15:05.917655Z",
     "iopub.status.busy": "2024-09-21T13:15:05.917493Z",
     "iopub.status.idle": "2024-09-21T13:15:05.924294Z",
     "shell.execute_reply": "2024-09-21T13:15:05.924052Z"
    },
    "papermill": {
     "duration": 2.50211,
     "end_time": "2024-09-21T13:15:05.924842",
     "exception": false,
     "start_time": "2024-09-21T13:15:03.422732",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추론 결과가 output_code8_nf.csv 파일로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 예측 결과를 CSV 파일로 저장\n",
    "test_info.to_csv(\"output_code8_nf.csv\", index=False)\n",
    "print(f\"추론 결과가 output_code8_nf.csv 파일로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac79e3df-e5c3-4a49-b37e-0dea1300c317",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:15:10.815977Z",
     "iopub.status.busy": "2024-09-21T13:15:10.815792Z",
     "iopub.status.idle": "2024-09-21T13:15:10.818903Z",
     "shell.execute_reply": "2024-09-21T13:15:10.818549Z"
    },
    "papermill": {
     "duration": 2.36276,
     "end_time": "2024-09-21T13:15:10.819495",
     "exception": false,
     "start_time": "2024-09-21T13:15:08.456735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def visualize_gradcam(\n",
    "#         model: torch.nn.Module,\n",
    "#         device: torch.device,\n",
    "#         dataloader: DataLoader,\n",
    "#         target_layer: str,\n",
    "#         image_index: int\n",
    "#     ):\n",
    "\n",
    "#     # Grad-CAM 추출기를 초기화합니다.\n",
    "#     cam_extractor = GradCAM(model, target_layer)\n",
    "    \n",
    "#     model.eval()  # 모델을 평가 모드로 설정합니다.\n",
    "#     fig, axes = plt.subplots(1, 3, figsize=(18, 6))  # 시각화를 위한 Figure를 생성합니다.\n",
    "    \n",
    "#     # 데이터 로더에서 배치를 반복합니다.\n",
    "#     current_index = 0\n",
    "#     for inputs in dataloader:\n",
    "#         inputs = inputs.to(device)  # 입력 이미지를 장치로 이동합니다.\n",
    "        \n",
    "#         outputs = model(inputs)  # 모델을 통해 예측을 수행합니다.\n",
    "#         _, preds = torch.max(outputs, 1)  # 예측된 클래스 인덱스를 가져옵니다.\n",
    "        \n",
    "#         # 배치 내의 각 이미지에 대해 처리합니다.\n",
    "#         for j in range(inputs.size()[0]):\n",
    "#             if current_index == image_index:\n",
    "#                 # CAM을 가져옵니다.\n",
    "#                 cam = cam_extractor(preds[j].item(), outputs[j].unsqueeze(0))[0]\n",
    "#                 # CAM을 1채널로 변환합니다.\n",
    "#                 cam = cam.mean(dim=0).cpu().numpy()\n",
    "                \n",
    "#                 # CAM을 원본 이미지 크기로 리사이즈합니다.\n",
    "#                 cam = cv2.resize(cam, (inputs[j].shape[2], inputs[j].shape[1]))\n",
    "                \n",
    "#                 # CAM을 정규화합니다.\n",
    "#                 cam = (cam - cam.min()) / (cam.max() - cam.min())  # 정규화\n",
    "#                 ㅈ\n",
    "#                 # CAM을 0-255 범위로 변환합니다.\n",
    "#                 cam = np.uint8(255 * cam)\n",
    "#                 # 컬러맵을 적용하여 RGB 이미지로 변환합니다.\n",
    "#                 cam = cv2.applyColorMap(cam, cv2.COLORMAP_JET)\n",
    "#                 cam = cv2.cvtColor(cam, cv2.COLOR_BGR2RGB)  # BGR에서 RGB로 변환\n",
    "                \n",
    "#                 # 입력 이미지가 1채널 또는 3채널인지 확인하고 처리합니다.\n",
    "#                 input_image = inputs[j].cpu().numpy().transpose((1, 2, 0))\n",
    "#                 if input_image.shape[2] == 1:  # 1채널 이미지인 경우\n",
    "#                     input_image = np.squeeze(input_image, axis=2)  # (H, W, 1) -> (H, W)\n",
    "#                     input_image = np.stack([input_image] * 3, axis=-1)  # (H, W) -> (H, W, 3)로 변환하여 RGB처럼 만듭니다.\n",
    "#                 else:  # 3채널 이미지인 경우\n",
    "#                     input_image = (input_image - input_image.min()) / (input_image.max() - input_image.min())\n",
    "#                     input_image = (input_image * 255).astype(np.uint8)  # 정규화된 이미지를 8비트 이미지로 변환합니다.\n",
    "                \n",
    "#                 # 오리지널 이미지\n",
    "#                 axes[0].imshow(input_image)\n",
    "#                 axes[0].set_title(\"Original Image\")\n",
    "#                 axes[0].axis('off')\n",
    "                \n",
    "#                 # Grad-CAM 이미지\n",
    "#                 axes[1].imshow(cam)\n",
    "#                 axes[1].set_title(\"Grad-CAM Image\")\n",
    "#                 axes[1].axis('off')\n",
    "                \n",
    "#                 # 오버레이된 이미지 생성\n",
    "#                 overlay = cv2.addWeighted(input_image, 0.5, cam, 0.5, 0)\n",
    "#                 axes[2].imshow(overlay)\n",
    "#                 axes[2].set_title(\"Overlay Image\")\n",
    "#                 axes[2].axis('off')\n",
    "                \n",
    "#                 plt.show()  # 시각화를 표시합니다.\n",
    "#                 return\n",
    "#             current_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91cd0879",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:15:15.945901Z",
     "iopub.status.busy": "2024-09-21T13:15:15.945712Z",
     "iopub.status.idle": "2024-09-21T13:15:15.948188Z",
     "shell.execute_reply": "2024-09-21T13:15:15.947852Z"
    },
    "papermill": {
     "duration": 2.515503,
     "end_time": "2024-09-21T13:15:15.948761",
     "exception": false,
     "start_time": "2024-09-21T13:15:13.433258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ace35397",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-21T13:15:20.978295Z",
     "iopub.status.busy": "2024-09-21T13:15:20.978106Z",
     "iopub.status.idle": "2024-09-21T13:15:20.980470Z",
     "shell.execute_reply": "2024-09-21T13:15:20.980024Z"
    },
    "papermill": {
     "duration": 2.516644,
     "end_time": "2024-09-21T13:15:20.981054",
     "exception": false,
     "start_time": "2024-09-21T13:15:18.464410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# target_layer = 'layer4.1.act2'\n",
    "\n",
    "# # Grad-CAM 시각화 실행 (예: 인덱스 3의 이미지를 시각화)\n",
    "\n",
    "# image_index = 3\n",
    "\n",
    "# visualize_gradcam(model.model, device, test_loader, target_layer=target_layer, image_index=image_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 79200.838937,
   "end_time": "2024-09-21T13:15:24.281505",
   "environment_variables": {},
   "exception": null,
   "input_path": "code8_nf.ipynb",
   "output_path": "code8_nf_result.ipynb",
   "parameters": {},
   "start_time": "2024-09-20T15:15:23.442568",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
